---
layout: single
title: "Mastering Few-Shot Prompting in LangChain: A Comprehensive Guide to FewShotPromptWithTemplates for Enhanced LLM Outputs"
categories: langchain
---
# Mastering Few-Shot Prompting in LangChain: A Comprehensive Guide to FewShotPromptWithTemplates for Enhanced LLM Outputs

Large Language Models (LLMs) have revolutionized how we approach natural language processing tasks. However, getting an LLM to produce specific, high-quality outputs often requires sophisticated prompting techniques. Few-shot prompting—providing examples of desired inputs and outputs before asking the model to perform a task—is one of the most effective techniques in this domain. LangChain's `FewShotPromptWithTemplates` offers a powerful, flexible implementation of this approach.

In this guide, we'll explore how to leverage `FewShotPromptWithTemplates` to enhance your LLM outputs through structured few-shot learning.

## Understanding Few-Shot Prompting

Before diving into the implementation details, let's clarify what few-shot prompting is and why it's effective:

Few-shot prompting involves showing an LLM several examples of the task you want it to perform before asking it to handle a new instance. This approach helps the model understand the pattern and format of the expected output without requiring fine-tuning.

For example, if you want an LLM to classify customer feedback as positive or negative, you might provide several examples of feedback paired with the correct classification before asking it to classify a new piece of feedback.

## Introducing FewShotPromptWithTemplates

LangChain's `FewShotPromptWithTemplates` is a specialized prompt template that makes few-shot prompting more systematic and powerful. It inherits from `StringPromptTemplate` and implements the standard Runnable Interface, giving it access to methods like `with_config`, `with_retry`, and more.

The class allows you to:
- Format a collection of examples using a template
- Select examples dynamically based on the input
- Combine examples with prefix and suffix templates
- Handle variable types appropriately

## Key Components of FewShotPromptWithTemplates

Let's break down the essential components of this class:

1. **Example Template**: A `PromptTemplate` used to format individual examples
2. **Example Selector or Examples**: Either a collection of examples or a selector to choose examples dynamically
3. **Prefix Template**: A template to place before the examples
4. **Suffix Template**: A template to place after the examples
5. **Example Separator**: A string used to join the examples

## Basic Implementation

Here's a simple implementation of `FewShotPromptWithTemplates`:

```python
from langchain_core.prompts import FewShotPromptWithTemplates, PromptTemplate

# Define the template for formatting each example
example_template = PromptTemplate.from_template(
    "Input: {input}\nOutput: {output}"
)

# Define examples
examples = [
    {"input": "What is the capital of France?", "output": "Paris"},
    {"input": "Who wrote Romeo and Juliet?", "output": "William Shakespeare"},
    {"input": "What is the largest planet in our solar system?", "output": "Jupiter"}
]

# Define prefix and suffix templates
prefix_template = PromptTemplate.from_template(
    "Answer the following questions based on your knowledge:\n\n"
)
suffix_template = PromptTemplate.from_template(
    "\nInput: {query}\nOutput:"
)

# Create the few-shot prompt
few_shot_prompt = FewShotPromptWithTemplates(
    examples=examples,
    example_prompt=example_template,
    prefix=prefix_template,
    suffix=suffix_template,
    example_separator="\n\n"
)

# Format the prompt with a new query
formatted_prompt = few_shot_prompt.format(query="What is the boiling point of water?")
print(formatted_prompt)
```

This code will produce a prompt that includes the prefix, the formatted examples, and the suffix with the new query.

## Using Example Selectors for Dynamic Example Selection

One of the most powerful features of `FewShotPromptWithTemplates` is the ability to dynamically select examples based on the input using an `ExampleSelector`. This is particularly useful when you have a large collection of examples and want to choose the most relevant ones for each input.

```python
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

# Create an example selector
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    k=2  # Number of examples to select
)

# Create the few-shot prompt with the example selector
dynamic_few_shot_prompt = FewShotPromptWithTemplates(
    example_selector=example_selector,
    example_prompt=example_template,
    prefix=prefix_template,
    suffix=suffix_template,
    example_separator="\n\n"
)

# Format the prompt with a new query
# The selector will automatically choose the most relevant examples
formatted_prompt = dynamic_few_shot_prompt.format(query="What is the highest mountain in the world?")
print(formatted_prompt)
```

## Advanced Configuration Options

`FewShotPromptWithTemplates` supports several advanced configuration options:

### Type Handling

You can specify the types of variables the prompt template expects:

```python
few_shot_prompt = FewShotPromptWithTemplates(
    examples=examples,
    example_prompt=example_template,
    prefix=prefix_template,
    suffix=suffix_template,
    input_variables=["query"],
    partial_variables={"date": "2023-05-01"},
    validate_template=True,
    template_format="f-string"
)
```

### Partial Variables

You can provide partial variables that will be automatically included when formatting the prompt:

```python
from datetime import datetime

# Create a callable for dynamic partial variables
def get_current_date():
    return datetime.now().strftime("%Y-%m-%d")

few_shot_prompt = FewShotPromptWithTemplates(
    examples=examples,
    example_prompt=example_template,
    prefix=prefix_template,
    suffix=suffix_template,
    partial_variables={"current_date": get_current_date}
)
```

## Integration with LangChain Chains

`FewShotPromptWithTemplates` integrates seamlessly with LangChain's chain architecture:

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Initialize language model
model = ChatOpenAI(temperature=0)

# Create a chain
chain = (
    {"query": RunnablePassthrough()}
    | few_shot_prompt
    | model
    | StrOutputParser()
)

# Run the chain
result = chain.invoke("What is the speed of light?")
print(result)
```

## Handling Streaming Responses

The `FewShotPromptWithTemplates` class implements both `stream` and `astream` methods, allowing you to work with streaming responses from LLMs:

```python
async def process_streaming_response():
    model = ChatOpenAI(temperature=0, streaming=True)
    
    chain = (
        {"query": RunnablePassthrough()}
        | few_shot_prompt
        | model
        | StrOutputParser()
    )
    
    async for chunk in chain.astream("Explain quantum computing"):
        print(chunk, end="", flush=True)

# In an async context
# await process_streaming_response()
```

## Error Handling and Fallbacks

You can add fallbacks to your chains using the `with_fallbacks` method:

```python
from langchain_core.runnables import RunnableWithFallbacks

# Create a fallback chain
fallback_model = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
fallback_chain = (
    {"query": RunnablePassthrough()}
    | few_shot_prompt
    | fallback_model
    | StrOutputParser()
)

# Add fallback to the main chain
robust_chain = RunnableWithFallbacks(
    chain,
    fallbacks=[fallback_chain]
)

# The chain will use the fallback if the primary model fails
result = robust_chain.invoke("What is the theory of relativity?")
```

## Saving and Loading Prompts

You can save your few-shot prompts to reuse them later:

```python
import json
from pathlib import Path

# Save examples to a JSON file
with open("examples.json", "w") as f:
    json.dump(examples, f)

# Save the prompt template
few_shot_prompt.save("few_shot_prompt.json")

# Later, you can load it back
# Note: This is a simplified example; actual loading would require more steps
```

## Best Practices for Few-Shot Prompting

To get the most out of `FewShotPromptWithTemplates`, follow these best practices:

1. **Choose diverse examples**: Include a range of examples that cover different aspects of the task.
2. **Order matters**: The order of examples can influence the model's understanding, so experiment with different arrangements.
3. **Be consistent**: Maintain consistent formatting across examples.
4. **Keep it simple**: Don't overcomplicate the example template; clarity is key.
5. **Use semantic selectors**: For large example sets, use semantic selectors to automatically choose the most relevant examples.

## Conclusion

LangChain's `FewShotPromptWithTemplates` provides a flexible, powerful framework for implementing few-shot prompting with LLMs. By structuring your examples and leveraging dynamic selection, you can significantly improve the quality and consistency of your LLM outputs.

Whether you're building a classification system, a question-answering service, or any other NLP application, few-shot prompting can help you get more reliable, more accurate results from your language models. The `FewShotPromptWithTemplates` class makes implementing this technique straightforward and adaptable to a wide range of use cases.

## Example: Building a Customer Support Response Generator

Let's conclude with a practical example—a system that generates customer support responses based on a few examples:

```python
from langchain_core.prompts import FewShotPromptWithTemplates, PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Define the example template
example_template = PromptTemplate.from_template(
    "Customer: {customer_message}\nSupport: {support_response}"
)

# Define examples
examples = [
    {
        "customer_message": "I haven't received my order yet and it's been a week.",
        "support_response": "I apologize for the delay. Let me check the status of your order. Could you please provide your order number?"
    },
    {
        "customer_message": "The product I received is damaged.",
        "support_response": "I'm sorry to hear that your product arrived damaged. We'll make this right. Could you please send photos of the damage and your order number?"
    },
    {
        "customer_message": "How do I reset my password?",
        "support_response": "To reset your password, please go to the login page and click on 'Forgot Password'. You'll receive an email with instructions to create a new password."
    }
]

# Define prefix and suffix templates
prefix_template = PromptTemplate.from_template(
    "You are a helpful customer support agent. Respond to customer inquiries professionally and helpfully.\n\n"
)
suffix_template = PromptTemplate.from_template(
    "\nCustomer: {customer_message}\nSupport:"
)

# Create the few-shot prompt
support_prompt = FewShotPromptWithTemplates(
    examples=examples,
    example_prompt=example_template,
    prefix=prefix_template,
    suffix=suffix_template,
    example_separator="\n\n"
)

# Create the chain
model = ChatOpenAI(temperature=0.7)
support_chain = (
    {"customer_message": RunnablePassthrough()}
    | support_prompt
    | model
    | StrOutputParser()
)

# Test the chain
customer_message = "I want to return an item I purchased last week."
response = support_chain.invoke(customer_message)
print(f"Customer: {customer_message}")
print(f"Support: {response}")
```

This example demonstrates how `FewShotPromptWithTemplates` can be used to create a practical application that leverages few-shot learning to generate contextually appropriate responses.

By mastering `FewShotPromptWithTemplates`, you'll be able to harness the full power of few-shot learning in your LLM applications, leading to more accurate, more consistent, and more useful outputs.


**This post was originally written in my native language and then translated using an LLM. I apologize if there are any grammatical inconsistencies.**
